import os
import sys
import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
import pandas as pd
from SentenceTokeniser import SentenceTokeniser
from keras.preprocessing import sequence
from keras.optimizers import SGD, RMSprop, Adagrad
from keras.utils import np_utils
from keras.models import Sequential,Model
from keras.layers.core import Dense, Dropout, Activation,Flatten
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM, GRU
from keras.layers.convolutional import Conv1D,MaxPooling1D
from keras.layers import Input
from keras.datasets import imdb
from keras import backend as K
from keras.models import model_from_json
from csv import DictReader
import couchdb
from CouchDBConnect import CouchDBConnect
import core
import h5py
import pickle
import Sentiment_twitter as S

MAX_SEQUENCE_LENGTH = 1000
MAX_NB_WORDS = 50000
EMBEDDING_DIM = 100
VALIDATION_SPLIT = 0.2
RUN = False
# first, build index mapping words in the embeddings set
# to their embedding vector

_db_handle=CouchDBConnect.connect_CouchDB()
db = _db_handle["tweets"]

def get_tweet_id(api, days_ago, query,geocode="Australia"):
    ''' Function that gets the ID of a tweet. This ID can then be
        used as a 'starting point' from which to search. The query is
        required and has been set to a commonly used word by default.
        The variable 'days_ago' has been initialized to the maximum
        amount we are able to search back in time (9).'''
    # return an ID from __ days ago
    td = dt.datetime.now() - dt.timedelta(days=days_ago)
    tweet_date = '{0}-{1:0>2}-{2:0>2}'.format(td.year, td.month, td.day)
    # get list of up to 10 tweets
    tweet = api.search(q=query, count=10,until=tweet_date,geocode=geocode)
    print("jvjbv",tweet)
    print('search limit (start/stop):', tweet[0].created_at)
    # return the id of the first tweet in the list
    return tweet[0].id

def get_text(db):
    queue = {}
    try:
        for row in db.view('senti/forSenti',
                                 wrapper=None,
                                 group_level=1):


                queue.update({row.key[0]: {}})
        for row in db.view('senti/forSenti',
                                 wrapper=None,
                                 group='true'
                                 ):
                queue[row.key[0]]['rev'] = row.key[1]
                queue[row.key[0]]['text'] = row.key[2]
    except:
        raise Exception("Failed to retrieve view: "
            + db.name
            + "/tweets/_view/forSenti\n\n")
    return queue
def iterate_doc(db):

    try:
        replies = get_text(db)
        for r in replies:
            try:
                doc = db.get(r)
                text = doc['text']
                print("b")
                sentiment1=S.SentimentCheck(text)
                print("a")
                if(sentiment1[0][1]>0.6) :
                    s="positive"
                elif (sentiment1[0][0]>0.6) :
                    s="negative"
                else :
                    s="neutral"

                doc['sentiment']=s
                doc = db.save(doc)
                # print (doc['_id'], doc['_rev'])
                print ("Saved")
            except:
                print("Exception1")
    except:
        print("Exception2")
        pass
def main():
  print
  _db_handle=CouchDBConnect.connect_CouchDB()
  db = _db_handle["tweets"]
  iterate_doc(db)
  doc = db.get("788943546422177792")
  text = doc['text']
  print("b")
  sentiment1=S.SentimentCheck(text)
  print("a")
  if(sentiment1[0][1]>0.6) :
    s="positive"
  elif (sentiment1[0][0]>0.6) :
    s="negative"
  else :
    s="neutral"

  doc['sentiment']=s
  doc = db.save(doc)

if __name__ == "__main__":
    main()
